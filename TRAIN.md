# 分阶段训练策略
## Stage 1：纯导航预训练（打地基）

### 目标提示词

在没有任何对抗信息的情况下，训练一个稳定、可泛化的导航 diffusion policy，确保模型在 g=0 时行为合理且可重复。

### 你需要确认的事情

#### 数据：

是否只使用纯导航或弱交互数据？

是否所有样本都显式设置 g = 0？

#### 模型：

是否只启用 E_nav + shared backbone + diffusion？

是否完全禁用或冻结 E_adv？

#### 训练：

是否确认 diffusion loss 能稳定下降？

是否观察到 g=0 时轨迹质量随训练提升？

#### 验证：

固定观测，采样多次，轨迹是否一致？

插值 z_nav，轨迹是否平滑变化？

#### 成功信号

当 g=0 时，模型表现出稳定、合理、可解释的导航行为。

## Stage 2：引入对抗意图（受控接入）

### 目标提示词

在不破坏导航能力的前提下，让模型学会使用 z_adv 对行为进行“局部、条件性的修正”。

### 你需要确认的事情

#### 数据：

是否同时使用 D_nav 和 D_adv？

是否正确标注：

D_nav → g = 0

D_adv → g = 1

#### 模型参数策略：

是否冻结或降低学习率：

shared backbone

E_nav

diffusion

是否只让 E_adv 正常学习？

#### Mask 机制：

是否在 latent 层明确实现 z = [z_nav, g * z_adv]？

是否确认 g=0 时梯度不会流入 E_adv？

#### 行为验证：

同一观测下：

g=0 行为 ≈ Stage 1

g=1 行为出现系统性偏移

固定 z_nav，扰动 z_adv 是否能改变轨迹？

#### 失败信号（警告）

如果 g=1 和 g=0 行为几乎一致，或 z_adv 改变不影响轨迹，说明对抗意图未被真正使用。

## Stage 3：联合微调 / 策略增强（可选）

### 目标提示词

在确认导航与对抗意图已成功解耦后，对整体系统进行小幅联合优化，提升对抗行为质量或鲁棒性。

### 你需要确认的事情

#### 前提检查：

z_nav 是否稳定、可解释？

z_adv 是否 controllable？

g 的语义是否清晰？

#### 训练策略：

是否使用极小学习率进行 joint fine-tuning？

是否避免破坏 Stage 1 的导航能力？

#### 可选增强：

是否加入对抗相关的 auxiliary objective？

是否在推理阶段尝试直接优化 z_adv？

#### 回归测试：

g=0 行为是否退化？

是否出现 z_nav / z_adv 信息泄漏？

#### 成功信号

系统在保持原有导航性能的同时，生成更具策略性的对抗行为。

---

# 数据集处理与标准格式

为了训练意图解耦模型，我们需要从标准的视觉导航数据集（ViNT/NoMaD格式）中提取或增强出对抗交互信息。

## 标准数据集格式规范

理想情况下，一个完全支持 ADSCD 框架的数据集应包含以下字段：

| 字段名 | 维度 | 描述 | 必要性 |
| :--- | :--- | :--- | :--- |
| **`obs_image`** | `[3*C, H, W]` | 上下文 RGB 图像（第一视点） | **必须** |
| **`action`** | `[H, A]` | 机器人的 Ground Truth 未来轨迹 (dx, dy) | **必须** |
| **`goal_image`** | `[3, H, W]` | 局部导航目标图像 | 推荐 (Vision Encoder需要) |
| **`other_state`** | `[S]` | 交互对象的相对状态 (如 [rel_x, rel_y, vel_x...]) | **必须** (仅在 g=1 时有效) |
| **`adv_mask` (g)** | `[1]` | 意图开关标签。0=纯导航，1=对抗/交互模式 | **必须** |

## 现有数据集的适配方案

由于大多数开源数据集（如 GoStanford, TartanDrive）仅针对单体导航设计，缺少交互标签，我们采用了**Sidecar Metadata（外挂索引）**方案进行非侵入式适配。

### 1. 纯导航数据集 (如 GoStanford)

此数据集完全没有动态障碍物或交互对象。使用默认处理：

*   **处理脚本**: 无需额外脚本。
*   **加载逻辑**: `train_adscd.py` 中的 `ADSCD_DatasetWrapper` 会自动检测是否缺少 metadata。
*   **默认值**: 
    *   `adv_mask = 0` (强制进入纯导航模式)
    *   `other_state = [0, 0]` (Zero padding)
*   **用途**: 用于 **Stage 1** 训练纯视觉特征提取器和导航策略。

### 2. 混合/自制数据集 (如 Gazebo 仿真数据)

此数据集包含部分交互场景，但原始格式仍为 NoMaD 结构（图片文件夹 + 轨迹）。

*   **处理脚本**: `scripts/build_adv_metadata.py`
    *   该脚本遍历数据集，根据文件名规则、ROS日志或视觉检测结果，生成一个独立的元数据文件（`.pkl`）。
*   **元数据内容**: 字典结构 `{(traj_name, time_step): {'adv_mask': 1.0, 'other_state': [x, y]}, ...}`
*   **加载逻辑**: `train_adscd.py` 读取 `.pkl` 文件。
    *   如果当前帧在 metadata 中有记录 -> 使用记录值（g=1 或 g=0）。
    *   如果无记录 -> 回退到默认值 (g=0)。
*   **用途**: 用于 **Stage 2** 和 **Stage 3** 训练对抗意图模块 ($z_{adv}$)。

### 为什么这样做？

1.  **兼容性**: 我们可以复用现成的大规模导航数据集（TB级数据）作为 Base，而不需要重新处理所有的 TFRecord 或 LMDB。
2.  **灵活性**: 对抗标签的定义可能会变（例如从基于位置改为基于碰撞风险），外挂索引允许我们随时重新生成标签，而无需触碰庞大的原始图像数据。